# Question Generation using ðŸ¤— transformers

## Project Details
This project is for purpose of education and learning. This question generation task is mainly for generating question-answer pair for given text or context.

### Initial experiments
Initial experiments are conducted using the SQuADv2 dataset and T5 model. This dataset is already in Indonesian and English languages. The Indonesian version of SQuAD is provided by Wikidepia and can be fetched at [this link](https://github.com/Wikidepia/indonesian_datasets).


### Answer styles
The system can generate questions with full-sentence answers `('sentences')`, questions with multiple-choice answers `('multiple_choice')`, or a mix of both `('all')`.

## Models
There are 2 models that we use. First one for generating questions. The second for evaluating the question and answer

### Question Generator
[Question Generator HF Model](https://huggingface.co/widyanto/IndoT5-small-qg)<br>
The question generator model takes a text as input and outputs a series of question and answer pairs. The answers are sentences and phrases extracted from the input text. The extracted phrases can be either full sentences or named entities extracted using [cahya/bert-base-indonesian-NER](https://huggingface.co/cahya/bert-base-indonesian-NER). Named entities are used for multiple-choice answers. The wrong answers will be other entities of the same type found in the text. The questions are generated by concatenating the extracted answer with the full text (up to a maximum of 512 tokens) as context in the following format:

```
answer_token <extracted answer> context_token <context>
```
The concatenated string is then encoded and fed into the question generator model. The model architecture is `IndoT5-small`. The datasets were restructured by concatenating the answer and context fields into the previously mentioned format. The concatenated answer and context was then used as an input for training, and the question field became the targets.

The datasets can be found [here](https://drive.google.com/drive/folders/13FjYisqRHkTRWWmtRpuF4-s9s_GxvJs2?usp=sharing).

### QA Evaluator
[QA Evaluator HF Model](https://huggingface.co/widyanto/indobert-base-uncased-qa-evaluator)<br>
The QA evaluator takes a question answer pair as an input and outputs a value representing its prediction about whether the input was a valid question and answer pair or not. The model is `indolem/indobert-base-uncased` with a sequence classification head. The pretrained model was finetuned on the same data as the question generator model, but the context was removed. The question and answer were concatenated 50% of the time. In the other 50% of the time a corruption operation was performed (either swapping the answer for an unrelated answer, or by copying part of the question into the answer). The model was then trained to predict whether the input sequence represented one of the original QA pairs or a corrupted input.

The input for the QA evaluator follows the format for `BertForSequenceClassification`, but using the question and answer as the two sequences. It is the following format:

```
[CLS] <question> [SEP] <answer [SEP]
```

## Relevant papers
- https://arxiv.org/abs/1906.05416
- https://www.aclweb.org/anthology/D19-5821/
- https://arxiv.org/abs/2005.01107v1
